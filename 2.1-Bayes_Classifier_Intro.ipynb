{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#plot.rcParams['figure.figsize'] = [4, 4]\n",
    "histogram_bins = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Wells Lucas Santo from AI4ALL. Some of the material here is based on his slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "*Probability* is a tool for describing how uncertain or how likely an event will happen.\n",
    "\n",
    "For example, if you flip a (fair) coin, what is the probability of it landing tails?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an event is expressed as a number between 0 and 1.\n",
    "\n",
    "Probability = 0: impossible.\n",
    "\n",
    "Probability = 1: certain to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a coin flip, the possible outcomes are \"heads\" and \"tails\". The probability is 0.5 for heads, and 0.5 for tails.\n",
    "\n",
    "In mathematical notation:\n",
    "\n",
    "> *P*(heads) = 0.5\n",
    ">\n",
    "> *P*(tails) = 0.5\n",
    "\n",
    "(Actually it's probably something like *P*(heads) = 0.4999, *P*(tails) = 0.4999, *P*(edge) = 0.0002, but let's ignore that.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percents\n",
    "\n",
    "You could also express probabilities as percentages: just multiply by 100.\n",
    "\n",
    "> P(heads) = 50%\n",
    "\n",
    "But we'll usually use numbers 0&ndash;1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "We sometimes have to talk about the probability of two related things happening: what is the probability of X happening if we know Y has happened?\n",
    "\n",
    "We write that as:\n",
    "\n",
    "> *P*(X | Y)\n",
    "\n",
    "Pronounce that at \"the probability of X given Y\" or \"the probability of X if Y\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example: if we select a person at random, what is the probability that the person is attending InventTheFuture?\n",
    "\n",
    "> *P*(ITF) = 24/7000000000 = 0.0000000034\n",
    "\n",
    "What is the probability of select a person at random, what is the probability of that the person is attending InventTheFuture, if we know **the person is in this room**?\n",
    "\n",
    "> *P*(ITF | InThisRoom) = 24/28 = 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24./7000000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24./28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Events\n",
    "\n",
    "Some events are related to each other, and the conditional probability doesn't tell us anything. What if we know it's raining, and we select a person. Then what is the probability of them attending InventTheFuture?\n",
    "\n",
    "Same as before: the weather is independent of coming to ITF.\n",
    "\n",
    "> *P*(ITF | ItsRaining) = *P*(ITF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem\n",
    "\n",
    "Bayes Theorem gives us a way to flip around a conditional probability: \n",
    "\n",
    "> *P*(Y | X) = *P*(X | Y) \\* *P*(Y) / *P*(X)\n",
    "\n",
    "If we know something about *P*(X | Y), we can use that to get to *P*(Y | X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example:\n",
    "\n",
    "> *P*(InThisRoom | ITF) = *P*(ITF | InThisRoom) \\* *P*(InThisRoom) / *P*(ITF) = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(24./28.) * (28./7000000000.) / (24./7000000000.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're in ITF, it's certain that you're in this room. It doesn't seem like we had enough information to calculate that, but Bayes theorem told us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Our Goal\n",
    "\n",
    "Let's remember what we're trying to do here:\n",
    "\n",
    "*Supervised Learning*: learning from labelled examples.\n",
    "\n",
    "*Classification*: A type of supervised learning problem where we want to determine what class each example belongs to.\n",
    "\n",
    "*Features*: the input we are using. Whatever was observed/measured.\n",
    "\n",
    "*Label*: the thing we're trying to predict, the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we want to have for a machine learning classification problem:\n",
    "\n",
    "1. Start with some labelled data where we know the features **and** correct labels.\n",
    "1. Split that data into two parts: **training** data and **testing** data.\n",
    "1. Create a **model** which will be a general strategy for mapping features to labels.\n",
    "1. Use the training data to **fit** the model to our problem, so it can predict correct labels.\n",
    "1. Use the testing data to evaluate our model and give it a **score**: how well do the model's predictions match the correct labels on the testing data?\n",
    "1. Once we have a good model, use it to **predict** labels for data where we don't know the correct result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions with Probability\n",
    "\n",
    "The whole goal is to create a classifier that can take a data point (features) and predict a label from the possibilities.\n",
    "\n",
    "Idea: let's use probability and Bayes Theorem for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're thinking about probability, we want to start with the data and come up with a probability that we're in each class.\n",
    "\n",
    "If we can come up with a probability that we're in each class, we can just find the highest probability and predict that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probabilty notation, we want to know the probability of being in a particular class C, given that we have observed data X.\n",
    "\n",
    "> *P*(C | X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem gives us a way to use *P*(X | C) to figure that out. All we need is some way to figure out the probability of particular observations, given the true class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distributions\n",
    "\n",
    "Any time we have something random happening where a number is produced, we can ask how those numbers are distributed: the **probability distribution**.\n",
    "\n",
    "As an example: suppose we flip a coin 1000 times. How many heads will we see?\n",
    "\n",
    "Sometimes 500. Sometimes 499. Very rarely, 100. We would like some way to express the way those results are distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the experiment: we will flip 1000 coins and count the number of heads. We'll repeat the experiment many times and see what the outcomes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "observations = []\n",
    "n = 1000\n",
    "repeat = 2000\n",
    "for experiment in range(repeat): # repeat the experiment 2000 times\n",
    "    total_heads = 0\n",
    "    for coin in range(n):    # flip 1000 coins\n",
    "        result = random.choice(['H', 'T'])\n",
    "        if result == 'H':\n",
    "            total_heads += 1\n",
    "    observations.append(total_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster version of the experiment if we want larger numbers...\n",
    "#repeat = 10000\n",
    "#n = 1000\n",
    "#observations = numpy.random.randint(0, 2, (repeat, n)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = numpy.array(observations)\n",
    "histogram_data = plot.hist(observations, bins=histogram_bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat that experiment many times (increasing the 2000 times above), that histogram will look more and more like a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).\n",
    "\n",
    "This one, actually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "x = numpy.linspace(450, 550, 100)\n",
    "mean = 0.5*n\n",
    "stddev = (0.25*n) ** 0.5\n",
    "plot.plot(x, norm.pdf(x, mean, stddev), 'b-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal distributions happen in many, many places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normally-Distributed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive Bayes classifer assumes that our observations (for each class) follow a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).\n",
    "\n",
    "In the example above, we knew the distribution because of some mathematics about random distributions in situations like flipping coins. We don't usually know the \"true\" distribution: we have to make our best guess what it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring Out the Distribution\n",
    "\n",
    "There are two things we need to know about a normal distribution: its **mean** and **standard deviation**.\n",
    "\n",
    "The *mean* is the average of the values.\n",
    "\n",
    "The *standard deviation* is a way to measure the &ldquo;width&rdquo; of the data: how spread out is it?\n",
    "\n",
    "We can calculate both of those: they will tell us exactly what normal distribution the data seems to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = observations.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = observations.std()\n",
    "stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check how close we are: we can draw the normal distribution that we discovered (the red line) on top of the histogram of the data points we started with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.hist(observations, bins=histogram_bins)\n",
    "x = numpy.linspace(observations.min(), observations.max(), 100)\n",
    "normal_curve = norm.pdf(x, mean, stddev)\n",
    "plot.plot(x, normal_curve / normal_curve.max() * histogram_data[0].max(), 'r-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Classes\n",
    "\n",
    "We are more concerned about having more than one class of observations, and we want to be able to predict for future observations which class they are in.\n",
    "\n",
    "Imagine we have two classes: blue and red. We will make up some fake data and see how it looks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "observations_a = numpy.random.normal(23, 19, n)\n",
    "observations_b = numpy.random.normal(91, 24, n)\n",
    "\n",
    "histogram_data = plot.hist([observations_a, observations_b],\n",
    "                           bins=histogram_bins, color=['b','r'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same trick as before and guess a normal distribution for each class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_a = observations_a.mean()\n",
    "mean_b = observations_b.mean()\n",
    "stddev_a = observations_a.std()\n",
    "stddev_b = observations_b.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.hist([observations_a, observations_b], bins=histogram_bins, color=['b','r'])\n",
    "x_range = numpy.linspace(observations_a.min(), observations_b.max(), 1000)\n",
    "curve_a = norm.pdf(x_range, mean_a, stddev_a)\n",
    "curve_b = norm.pdf(x_range, mean_b, stddev_b)\n",
    "plot.plot(x_range, curve_a / curve_a.max() * histogram_data[0][0].max(), 'b-', lw=2)\n",
    "plot.plot(x_range, curve_b / curve_a.max() * histogram_data[0][0].max(), 'r-', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilities and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done so far:\n",
    "1. Make some observations about individuals in different classes: our labeled data.\n",
    "1. Know something about the data, and have a pretty good idea that each class' measurements are going to be normally distributed.\n",
    "1. Calculate the mean and standard deviation.\n",
    "1. Use those to guess a normal distribution for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution gives us a ways to guess the probability of getting each observation, if we know what class we are looking at.\n",
    "\n",
    "Or in other words, if we measured X for something in class C, the probability distribution tells us\n",
    "> *P*(X | C)\n",
    "\n",
    "If we want to start making prediction on unlabeled data, we need to figure out what the most-likely class is for the observations we made, or\n",
    "> *P*(C | X)\n",
    "\n",
    "Bayes theorem gives us a way to turn one into the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't have to do the calculations ourselves... there are tools for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Features\n",
    "\n",
    "Those examples assumes we were only measuring one thing with each observation. We can measure any number of things. The picture (and math) just gets a little more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions we're going to make:\n",
    "1. The values that we measure are **independent**: if we measure height and age, those don't have anything to do with each other.\n",
    "2. Each thing we measure has a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what happens if we measure two features with some more sample data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "x = numpy.random.normal(10, 3, n)\n",
    "y = numpy.random.normal(3, 1, n)\n",
    "plot.plot(x, y, 'b.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're assuming that the features are independent, we can deal with them separately just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.bayes import joint_histograms, pdf_2d\n",
    "joint_histograms(numpy.stack([x,y], axis=1), numpy.zeros((n,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have multiple classes, we do the same thing with each one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.bayes import cmap, sample_data_1\n",
    "observations, classes = sample_data_1()\n",
    "plot.scatter(observations[:, 0], observations[:, 1], c=classes/2.0, cmap=cmap, edgecolor='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_histograms(observations, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like before, we calculate the mean and standard deviation of the data for each class: this time for each feature as well.\n",
    "\n",
    "Then we get normal distributions for each class and feature. That gives us a probability that each point is in a particular class.\n",
    "\n",
    "This is the probability of being in class 0 (blue), with brightness representing higher probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_2d(observations, classes, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "Once we have figured out these probability distributions, we can use them to make predictions. The things we're doing are now something like:\n",
    "\n",
    "1. Make some observations about individuals in different classes: our labeled data.\n",
    "1. Know something about the data, and have a pretty good idea that each class' measurements are going to be normally distributed.\n",
    "1. Calculate the mean and standard deviation.\n",
    "1. Use those to guess a normal distribution for each class, **for each feature separately**. Combine those distributions so we can guess a value for *P*(X | C).\n",
    "1. Use Bayes theorem (behind the scenes) to calculate *P*(C | X). When we want to make a prediction for the class X is in, look at the *P*(C | X) values for each class.\n",
    "1. Choose the class with the **highest** *P*(C | X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math isn't *that* hard, but we don't actually have to do it ourselves..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
