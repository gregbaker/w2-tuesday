{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas\n",
    "import matplotlib.pyplot as plot\n",
    "from helpers.bayes import cmap, sample_data_1, plot_decision, joint_histograms\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#plot.rcParams['figure.figsize'] = [4, 4]\n",
    "\n",
    "# from https://stackoverflow.com/a/21009774/6871666\n",
    "numpy.set_printoptions(formatter={'float_kind': lambda x: \"%.4f\" % x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the same sample data: two features and points in three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, classes = sample_data_1()\n",
    "plot.scatter(observations[:, 0], observations[:, 1], c=classes/2.0, cmap=cmap, edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "We will use Scikit-Learn to give us a Gaussian Naive Bayes classifier to work with the data.\n",
    "\n",
    "1. **Gaussian**: synonym for \"normally\" distributed. We will assume the data (in one category on one feature) is normally distributed.\n",
    "1. **Naive**: we assume that the values for features are independent. The probability of a certain value for feature #1 doesn't change if you know the values for the other features.\n",
    "1. **Bayes**: we will use the conditional probability tricks from before and pick the class with the highest probability for the observation we're considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a Scikit-Learn model that will understand the world with the Gaussian Naive Bayes assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: Training and Testing\n",
    "\n",
    "We usually call our observations \"X\" and labeled classes \"y\", because it's easier to spell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = observations\n",
    "y = classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our X values are in an array with *n* rows (for *n* observations) and 2 columns, because there are two features in our data set.\n",
    "\n",
    "The y values are the *n* corresponding classes/labels that we know for those observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review: we want to use *some* of our data to train the model, and some to *test* the model so we can see how it will do on data it hasn't seen before (and isn't just memorizing the training set).\n",
    "\n",
    "There is a function in Scikit-Learn that will randomly break up our data into training and testing sets (75%, 25% by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the split is random, we might get *slightly* different results each time we run the program, but it will be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model\n",
    "\n",
    "Now that we have a model and some training data, we can **train** the model (or **fit** it) on that data, so it learns what kind of decisions to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the decisions it makes. This is the **training** data, and the background colours are the **predictions** that the model will make on each point if it is asked to guess the class at that observation.\n",
    "\n",
    "We can see how the training points guided the way the predictions are made, and where the boundaries are between the classes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing The Model\n",
    "\n",
    "The whole point of having the testing data is to use it to evaluate our model. That is a meaningful evaluation because the model didn't use this data as part of the training, so we think it would be representative of new unlabeled data we find later and want to make predictions about.\n",
    "\n",
    "This is what the **testing** data looks like on top of the predictions the model makes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can ask what fraction of the testing data the model predicts correctly: how many of the training X values are predicted to match the training y values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using The Model\n",
    "\n",
    "Once we have a trained model, we can use it to actually make some predictions.\n",
    "\n",
    "Let's pretend we have some new inputs and want to make our best guess what the category is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observations = numpy.array([\n",
    "    [4, 5],\n",
    "    [8, -10],\n",
    "    [2, -8],\n",
    "    [6, -1],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.predict` function can take these values and ask the model to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at those values on the plot (zoomed in slightly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(model, new_observations, model.predict(new_observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the naive Bayes classifier is working with probabilities: it calculates a probability for each class and selects the largest.\n",
    "\n",
    "We can ask for the probability for each class with `model.predict_proba`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(new_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the predictions on the third and fourth were close, but the model had to choose something."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
