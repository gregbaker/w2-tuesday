{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Language\n",
    "\n",
    "One common problem on the web: because it's world-wide, there are lots of natural languages out there. If you have some text (from a discussion forum or other message), it's not always obvious **which language** you have.\n",
    "\n",
    "In this exercise, we want to train a classifier to determine what natural language is used in a piece of natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, we need training data: somewhere that we have text in a variety of language, and know the language we're looking at.\n",
    "\n",
    "Fortunately, we have that: a random sample of tweets collected over the last year with [the Twitter API](https://developer.twitter.com/en/docs/tweets/sample-realtime/overview/GET_statuse_sample.html). Tweets come with their text (what the user actually tweeted) and the language that Twitter detected for that tweet. (Since the language we have it itself output of some machine learning algorithm, it's probably not perfect, but it's good enough for us to do some training with.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pandas.read_json(\"data/twitter-data.json.gz\", orient=\"records\", lines=True)\n",
    "tweets = tweets[['text', 'lang']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['text']\n",
    "y = tweets['lang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Split training and testing data as we have before. \n",
    "* Create a pipeline that has tf-idf transformation and a multinomial naive Bayes classifier.\n",
    "* Calculate a score (as we have before) to see what fraction is correct on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Out Your Results\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "Have a look at the `classification_report` output: where are you succeeding and failing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, model.predict(X_test), labels=model.classes_)\n",
    "seaborn.heatmap(confusion, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=model.classes_, yticklabels=model.classes_, cmap='viridis')\n",
    "plot.xlabel('predicted label')\n",
    "plot.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More About The Data\n",
    "\n",
    "How are we training this model anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data was weird: there are **many** English examples to train with, but very few Italian and Dutch (nl). Not surprisingly, the model doesn't have enough data to work with to correctly train the model.\n",
    "\n",
    "Go back and try with twitter-balanced.json.gz: that has a roughly-equal number of training points for each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe separating by words isn't the best way to get at the language? Maybe the distribution of **character** would be better. Maybe English uses more \"e\" than other languages and less \"q\"? This vectorizer will separate the text by character instead of by word. Maybe it's worth trying.\n",
    "```\n",
    "TfidfVectorizer(token_pattern=r'(?u)\\w')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything else you can think of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
