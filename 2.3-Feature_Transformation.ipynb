{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helpers.bayes import correlated_data, plot_decision, cmap, joint_histograms\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#plot.rcParams['figure.figsize'] = [4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features in Machine Learning\n",
    "\n",
    "Remember what we assumed about the features for Gaussian Naive Bayes: features are independent; features are normally distributed.\n",
    "\n",
    "If we violate those assumptions, the classifier might not work well.\n",
    "\n",
    "Let's have a look at some data where the features are not independent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = correlated_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "plot.scatter(X[:,0], X[:,1], c=y, edgecolor='k', cmap=cmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the decisions so bad?\n",
    "\n",
    "In the feature on the vertical axis, the mean and standard deviation are almost the same. The only difference is on the horizontal direction, so that's all the classifier can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_histograms(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "We need to manipulate the data as part of the machine learning process. We **don't always want to put our data directly into the model**: we often need to manipulate the data so that the model has something sensible it can work with.\n",
    "\n",
    "This is one of those cases: it would be nice if we could remove the correlation between the features before we let `GaussianNB` at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The things that we use to manipulate the data in Scikit-Learn are called **transformers**. These are tools that somehow manipulate the feature values and turn them into a new set of values that can be passed along to either another transformer or the model, an **estimator**.\n",
    "\n",
    "An **estimator** is a machine learning model that actually makes predictions. We have seen one: `GaussianNB`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put transformer(s) and an estimator together in a **pipeline**.\n",
    "\n",
    "A pipeline takes our observations (either the ones we're fitting with or making predictions on) and passes them through each step to an estimator where we get predictions.\n",
    "\n",
    "In Scikit-Learn, we can use `make_pipeline` to create a machine learning model includes several steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the `PCA` transformer. It does \"principlal component analysis\". We won't worry about the details, but one of the effects of PCA: it tends to remove correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `make_pipeline` to put out steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(\n",
    "    PCA(),\n",
    "    GaussianNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train and test a pipeline model just like any other Scikit-Learn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get *much* better predictions out of this model than `GaussianNB` by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can peek in the middle of the pipeline and see what the `PCA` transformer is doing in this case. This is the data after `PCA()` but before `GaussianNB`: the features still aren't truly independent, but things have been stretched enough that the Naive Bayes method can make some decent decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transf = model.named_steps['pca'].transform(X_train)\n",
    "plot.scatter(X_transf[:,0], X_transf[:,1], c=y_train, edgecolor='k', cmap=cmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_histograms(X_transf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers\n",
    "\n",
    "There are a few built-in transformers in Scikit-Learn, but there are many more ways you might want to manipulate your data.\n",
    "\n",
    "There is a transformer called `FunctionTransformer` that can be used to do whatever calculations you want to do with your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job: write a function that takes the X array (`X_train`, `X_test`, or whatever features matrix you give your model), does some calculations, and returns a new array of features.\n",
    "\n",
    "In this case, I make up this function (because I also made up the data, so I could cheat by knowing exactly what to do): it takes a matrix of features and returns some better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlation(X):\n",
    "    X0 = X[:, 0] - 1.5 * X[:, 1]\n",
    "    X1 = X[:, 1]\n",
    "    return numpy.stack((X0, X1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use this in a transformer in a pipeline. This pipeline model is really good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    FunctionTransformer(remove_correlation),\n",
    "    GaussianNB()\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can peek into the middle of the pipeline and see what happened to the (training) data when it went through the `FunctionTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transf = model.named_steps['functiontransformer'].transform(X_train)\n",
    "plot.scatter(X_transf[:,0], X_transf[:,1], c=y_train, edgecolor='k', cmap=cmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_histograms(X_transf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
